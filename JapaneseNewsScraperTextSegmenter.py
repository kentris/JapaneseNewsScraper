from tinysegmenter import TinySegmenter

hiragana = ["ぁ", "あ", "ぃ", "い", "ぅ", "う", "ぇ", "え", "ぉ", "お", "を", "か", "が", "き", "ぎ", "く", "ぐ", "け", "げ", "こ", "ご", "さ", "ざ", "し", "じ", "す", "ず", "せ", "ぜ", "そ", "ぞ", "た", "だ", "ち", "ぢ", "っ", "つ", "づ", "て", "で", "と", "ど", "な", "に", "ぬ", "ね", "の", "は", "ば", "ぱ", "ひ", "び", "ぴ", "ふ", "ぶ", "ぷ", "へ", "べ", "ぺ", "ほ", "ぼ", "ぽ", "ま", "み", "む", "め", "も", "ゃ", "や", "ゅ", "ゆ", "ょ", "よ", "ら", "り", "る", "れ", "ろ", "ゎ", "わ", "ゐ", "ゑ", "ん", "ゔ"]
numbers = ["１", "２", "３", "４", "５", "６", "７", "８", "９", "０"]
common_text = ['、', '。', '。\n', '\n', '・', '．', ' ', '。「', ' 「', '「', '、「', '」', '」\n', '（', '）', '＝', 'です', 'ます', 'まい', 'まり', 'まし', 'あり', 'ある', 'ない', 'いる', 'いない', 'この', 'これ', 'その', 'それ', 'あの', 'あれ', 'どの', 'どれ', 'まで',  'から', 'よう', '時', '分', 'して', 'する', 'すれ', 'また', 'たり', 'なる', 'なっ', 'なり', 'こと', 'られる', 'につ', 'ながる', 'なかっ', 'られ', 'れる', 'まか']

def parseText(text):
	t = TinySegmenter()
	allWords = t.tokenize(text)
	words = trimNonWords(allWords)
	return words

def trimNonWords(allWords):
	words = [word for word in allWords if word not in (hiragana + numbers + common_text)]
	return words


if __name__ == '__main__':
	pass