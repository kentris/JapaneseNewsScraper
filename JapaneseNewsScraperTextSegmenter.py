from tinysegmenter import TinySegmenter

hiragana = ["ぁ", "あ", "ぃ", "い", "ぅ", "う", "ぇ", "え", "ぉ", "お", "を", "か", "が", "き", "ぎ", "く", "ぐ", "け", "げ", "こ", "ご", "さ", "ざ", "し", "じ", "す", "ず", "せ", "ぜ", "そ", "ぞ", "た", "だ", "ち", "ぢ", "っ", "つ", "づ", "て", "で", "と", "ど", "な", "に", "ぬ", "ね", "の", "は", "ば", "ぱ", "ひ", "び", "ぴ", "ふ", "ぶ", "ぷ", "へ", "べ", "ぺ", "ほ", "ぼ", "ぽ", "ま", "み", "む", "め", "も", "ゃ", "や", "ゅ", "ゆ", "ょ", "よ", "ら", "り", "る", "れ", "ろ", "ゎ", "わ", "ゐ", "ゑ", "ん", "ゔ"]
letters = ['ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｋ', 'ｌ', 'ｍ', 'ｎ', 'ｏ', 'ｐ', 'ｑ', 'ｒ', 'ｓ', 'ｔ', 'ｕ', 'ｖ', 'ｗ', 'ｘ', 'ｙ', 'ｚ', 'Ａ', 'Ｂ', 'Ｃ', 'Ｄ', 'Ｅ', 'Ｆ', 'Ｇ', 'Ｈ', 'Ｉ', 'Ｊ', 'Ｋ', 'Ｌ', 'Ｍ', 'Ｎ', 'Ｏ', 'Ｐ', 'Ｑ', 'Ｒ', 'Ｓ', 'Ｔ', 'Ｕ', 'Ｖ', 'Ｗ', 'Ｘ', 'Ｙ', 'Ｚ']
numbers = ["１", "２", "３", "４", "５", "６", "７", "８", "９", "０", '一', '二', '三', '四', '五', '六', '七', '八', '九', '十', '千', '万']
common_text = ['、', '。', '。\n', '。\u3000', '\n', '\u3000', '・', '．', ' ', '。「', ' 「', '「', '、「', '「（', '」', '」\n', '」\u3000', '」「', '」（', '（', '）', '＝', '～', 'です', 'だっ', 'ます', 'まい', 'まり', 'まし', 'あり', 'ある', 'ない', 'いる', 'いない', 'おり', 'この', 'これ', 'その', 'それ', 'あの', 'あれ', 'どの', 'どれ', 'まで',  'から', 'よう', '時', '分', 'して', 'する', 'すれ', 'また', 'たり', 'なる', 'なっ', 'なり', 'こと', 'られる', 'たら', 'につ', 'ながる', 'なかっ', 'られ', 'れる', 'まか', 'あっ', 'たい' ]

def parseText(text):
	t = TinySegmenter()
	allWords = t.tokenize(text)
	words = trimNonWords(allWords)
	return words

def trimNonWords(allWords):
	words = [word for word in allWords if word not in (hiragana + letters + numbers + common_text)]
	return words


if __name__ == '__main__':
	pass